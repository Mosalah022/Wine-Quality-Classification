# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkZSBN2yox62bdCcIpEKskiIR40qhZCF

#Cover Page

**Developed by:**

**Mohamed Fathi - 41810059**

#Import the dataset from drive
"""

!gdown --id 1eBuFpD7VAbUCGwxfG5J70G3r2_Bkhqu8

"""#Importing libraries"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, classification_report, mean_absolute_error, confusion_matrix, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler,LabelEncoder

"""# show how the data looks like"""

data = pd.read_csv('winequality-white.csv',sep=';')
data.head(5)

data.describe()

data['quality label'] = data['quality'].apply(lambda x: 1 if x<=5 else 2 if x<=7 else 3)
print(data[['quality','quality label']].value_counts().sort_index())
data.head()

"""# Check if all data are numbers and if there is a relations between the data

"""

data.dtypes

data.info()

data.columns

"""# Check for the null """

data.isnull().sum()

"""# check for the outlier"""

from scipy import stats
z = np.abs(stats.zscore(data))

sns.boxplot(data['free sulfur dioxide'])

sns.boxplot(data['total sulfur dioxide'])

data.shape

data = data[(z < 3).all(axis=1)]

sns.boxplot(data['free sulfur dioxide'])

sns.boxplot(data['total sulfur dioxide'])

"""#Normalization"""

X = data.drop('quality',axis=1)
y = data['quality']

scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)
X.head()

print(X.shape)
print(y.shape)

"""#Start Split the data and train"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42, stratify=data['quality label'])
print(len(X_train), "train +", len(X_test), "valid")

"""**Using KNN ( k-nearest neighbors ) for trainning**

**check for the best K**
"""

for h in range(2, 21):
  print('neighbor number ', h)
  knn = KNeighborsClassifier(n_neighbors=h, weights='distance', algorithm='auto')
  knn.fit(X_train,y_train)
  print("knn train score is : ", knn.score(X_train, y_train))
  print("knn test score is : ", knn.score(X_test, y_test))
  print('-----------------------------------------------')

knn = KNeighborsClassifier(n_neighbors=16, weights='distance', algorithm='auto')
knn.fit(X_train,y_train)
y_pred = knn.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
cm

sns.heatmap(cm, annot=True)
plt.xlabel('Prediction')
plt.ylabel('Truth')

"""**Accuracy** """

var="%"
print("Accuracy: %0.1f" % (knn.score(X_test,y_test)*100), var[0])

"""# Classification Report"""

print(classification_report(y_test, y_pred))

