# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19YkguOFFbtrHKW6-nwGHAgpMmLmKJQPt

#Cover Page

**Developed by:**

**Fatma Mohamed Ali - 41810121**
"""

!gdown --id 1eBuFpD7VAbUCGwxfG5J70G3r2_Bkhqu8

"""#Description of data:

**The  datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, refer to [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.)**

**These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.**

#Content
Input variables (based on physicochemical tests):
1. fixed acidity
2. volatile acidity
3. citric acid
4. residual sugar
5. chlorides
6. free sulfur dioxide
7. total sulfur dioxide
8. density
9. pH
10. sulphates
11. alcohol
Output variable (based on sensory data):
12. quality (score between 0 and 10)

#Import the dataset from drive

#Importing libraries
"""

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns 
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn import svm
from sklearn import metrics

"""# show how the data looks like"""

df = pd.read_csv('winequality-white.csv',sep=';')
df.head()

df['quality label'] = df['quality'].apply(lambda x: 1 if x<=5 else 2 if x<=7 else 3)
print(df[['quality','quality label']].value_counts().sort_index())
df.head()

"""# Check if all data are numbers and if there is a relations between the data

"""

#df.dtypes
df.info()

df.columns

#from sklearn.preprocessing import LabelEncoder
#encoder = LabelEncoder()

"""#Heatmap"""

#df.corr()
x,y = plt.subplots(figsize=(12,9))
sns.heatmap(df.corr(),cmap='YlGnBu',square=True,linewidth=.5,annot=True)
plt.show()

"""# Check for the null """

#df.isna().any()
df.isnull().sum()
#df.dropna()

"""# show mean, max , min so handle the data if there is a gap between 75% and max  

"""

df.describe()

"""# check for the outlier"""

"""
df['Area'].skew()
df['Asymmetry.Coeff'].skew()
plt.boxplot(df['Kernel.Groove'])
plt.show()
"""
df.hist(bins=50,figsize=(20,15))
plt.show()

df.shape

"""
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
print (IQR)
df = df[~((df < (Q1 - 1.5 *  IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
print (df.shape)
"""

print(df.skew())

# plt.boxplot(dataset)      
# plt.show()

plt.boxplot(df['volatile acidity'])      
plt.show()

print((df).quantile(0.10))
print((df).quantile(0.90))

# print(df['volatile acidity'].skew())
# df["volatile acidity"]=np.where(df["volatile acidity"]< 0.170000, 0.170000,df["volatile acidity"])
# df["volatile acidity"]=np.where(df["volatile acidity"]> 0.40000,0.40000,df["volatile acidity"])
# print(df['volatile acidity'].skew())

plt.boxplot(df['volatile acidity'])      
plt.show()

print(df['citric acid'].skew())
df['citric acid']=np.where(df['citric acid']<  0.220000, 0.220000,df['citric acid'])
df['citric acid']=np.where(df['citric acid']> 0.490000,0.49000,df['citric acid'])
print(df['citric acid'].skew())

print(df['residual sugar'].skew())
df['residual sugar']=np.where(df['residual sugar']< 1.200000, 1.200000,df['residual sugar'])
df['residual sugar']=np.where(df['residual sugar']> 14.00000,14.00000,df['residual sugar'])
print(df['residual sugar'].skew())

print(df['free sulfur dioxide'].skew())
df['free sulfur dioxide']=np.where(df['free sulfur dioxide']< 15.000000, 15.000000,df['free sulfur dioxide'])
df['free sulfur dioxide']=np.where(df['free sulfur dioxide']>  57.00000, 57.00000,df['free sulfur dioxide'])
print(df['free sulfur dioxide'].skew())

print(df['chlorides'].skew())
df['chlorides']=np.where(df['chlorides']< 1.200000, 1.200000,df['chlorides'])
df['chlorides']=np.where(df['chlorides']> 14.00000,14.00000,df['chlorides'])
print(df['chlorides'].skew())

print(df.skew())

df.hist(bins=50,figsize=(20,15))
plt.show()

"""#Start Split the data and train"""

df_target = df["quality"]  #save target for training set
df = df.drop("quality", axis=1) #drop target for training set

"""#Normalization"""

scaler = StandardScaler()
df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
df.head()

"""**train_test_split imported from sklearn.model_selection**"""

"""
#split the data for training and validation
First Way to split data 
train_set_size= int(len (df) * 0.7)
train_set = df [:train_set_size][:]
valid_set = df [train_set_size:][:]

train_target = df_target [:train_set_size]
valid_target = df_target [train_set_size:]
print(len (train_set), "train +", len(valid_set), "valid")
"""
#second way
#train_set, valid_set = train_test_split(df, test_size=0.2)
#train_target, valid_target = train_test_split (df_target, test_size=0.2)


# print(df.shape)
# print(df_target.shape)

X_train, X_test, y_train, y_test = train_test_split(df, df_target, test_size = 0.20, random_state=22, stratify=df['quality label'])
print(len(X_train), "train +", len(X_test), "valid")

"""#Using **SVM** model for trainning

**Kernel = poly can be kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’**
"""

#Create a svm Classifier
clf = svm.SVC(kernel='poly') # poly

"""**Train the model using the training sets**

"""

clf.fit(X_train, y_train)

"""**Predict the response for test dataset**"""

y_pred = clf.predict(X_test)

"""**Import scikit-learn metrics module for accuracy calculation**

**Model Accuracy: how often is the classifier correct?**
"""

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""#find the mean square error

**mean_squared_error imported from from sklearn.metrics**

**np.sqrt imported from numpy**
"""

#lin_mse = mean_squared_error(Y_valid, y_pred)
#lin_rmse = np.sqrt(lin_mse)
#lin_rmse

"""**mean_absolute_error imported from from sklearn.metrics**

"""

#lin_mae = mean_absolute_error(Y_valid, y_pred)
#lin_mae

