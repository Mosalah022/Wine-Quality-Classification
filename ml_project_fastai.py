# -*- coding: utf-8 -*-
"""ML_Project_FastAi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l97wSTs6bV8GfleYOqaWEtaR-xJyWq2L
"""

#hide
#skip
! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab
!gdown --id 1eBuFpD7VAbUCGwxfG5J70G3r2_Bkhqu8

#hide
from fastai.tabular.all import *

#hide
filename = "winequality-white.csv"
path = Path("/content/")

"""#Cover Page
### Developed by:
*   Mohamed Salah Eldin - 41810303

# Problem Definition
**The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, refer to [Cortez et al., 2009]. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.)**

**These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.**

#Content
Input variables (based on physicochemical tests):
1. fixed acidity
2. volatile acidity
3. - citric acid
4.- residual sugar
5. - chlorides
6. - free sulfur dioxide
7. - total sulfur dioxide
8. - density
9. - pH
10. - sulphates
11. - alcohol
Output variable (based on sensory data):
12. - quality (score between 0 and 10)
"""

df = pd.read_csv(filename,sep=';')
df.head()

"""# Method
In order to accomplish accurate classification easily and quickly, we've opted for the FastAI library as it has excellent support for tabular data and makes cutting edge mdoels and techniques readily available.

We use 20% of the dataset for validation, and the seed has been set to 123 to ensure that the dataset is split in the same way each time the code is run, to ensure repeatability.

We've set it to normalize any continuous data (All fields), fill any missing fields and to categorify our variables (Type).

We've identified which columns are discrete and which are continuous too.
"""

seed = 123
use_seed = True;

splits = RandomSplitter(valid_pct=0.4, seed=seed)(range_of(df))
procs=[Categorify, FillMissing, Normalize]

cat_names=["quality"]
cont_names = ['fixed acidity','volatile acidity','citric acid','residual sugar',
           'chlorides','free sulfur dioxide','total sulfur dioxide', 'density','pH','sulphates','alcohol']

cont,cat = cont_cat_split(df, 1, dep_var='quality')

to = TabularPandas(df, procs=procs, cont_names=cont, cat_names=cat,
                   y_names='quality', splits=splits)

if use_seed: set_seed(seed, True)

"""[link text](https://)The batch size is set to 100. After experimentation we found that this batch size provided good accuracy after 10 epochs or so.

Decreasing the batch size leads to faster training and overfitting, but also gives over all lower accuracy.
"""

dls = to.dataloaders(bs=100)
if use_seed: dls.rng.seed(seed, True)

dls.show_batch()

"""# Experiment
A FastAI tabular learner object is instantiated. By default tabular learners have 2 hidden layers with 200 and 100 activations respectively, this was changed to 10000 and 100 activations respectively as that gave on average 1-2% better accuracy. Adding more hidden layers did not seem to provide any significant advantages.
"""

learn = tabular_learner(dls, metrics=accuracy_multi, layers=[10000,100])

"""lr_find does mock training over a large range of learning rates and graphs the loss for us. As per the recommendations in the fastai documentation for 1cycle, we pick a maximum learning rate that is an order of magnitude below the minima."""

learn.lr_find()

"""We're using the 1cycle technique to fit our model here, this is a relatively new technique that allows you to get convergence with less epochs. It works by varying the learning rate throughout the duration of the training."""

learn.fit_one_cycle(15, lr_max=1e-3)

"""We can see that overfitting starts happening after epoch 10~, though it is not significant."""

learn.recorder.plot_loss()

"""Below is a table that shows the predictions the trained model made on a few rows from the training dataset."""

learn.show_results()

"""Below some example code is provided that does inference on one row. Feel free to modify it."""

df.iloc[4800]

df.iloc[4800] = [7.00000,0.20000,0.33000,1.10000,0.03900,45.00000,126.00000,0.99051, 3.31000,0.45000,11.60000,5]

row, clas, probs = learn.predict(df.iloc[4800])

row.show()

clas, probs